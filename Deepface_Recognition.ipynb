{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0397562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "=== DeepFace Face Detection ===\n",
      "1. Analyze from image\n",
      "2. Analyze from video\n",
      "3. Analyze from webcam\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " áº¢nh káº¿t quáº£ Ä‘Ã£ lÆ°u táº¡i: result_deepface\\deepface_person1.jpg\n",
      "\n",
      "=== DeepFace Face Detection ===\n",
      "1. Analyze from image\n",
      "2. Analyze from video\n",
      "3. Analyze from webcam\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " áº¢nh káº¿t quáº£ Ä‘Ã£ lÆ°u táº¡i: result_deepface\\deepface_person2.jpg\n",
      "\n",
      "=== DeepFace Face Detection ===\n",
      "1. Analyze from image\n",
      "2. Analyze from video\n",
      "3. Analyze from webcam\n",
      "4. Exit\n",
      "ðŸ“¡ Äang phÃ¢n tÃ­ch tá»« webcam... Nháº¥n 'e' hoáº·c 'ESC' Ä‘á»ƒ thoÃ¡t.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.97it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.46it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.64it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.98it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.13it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.16it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.01it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.18it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.11it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.14it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.07it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.21it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.16it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.16it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.10it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.79it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.10it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.02it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.93it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.21it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.16it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.17it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.17it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.18it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.22it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.18it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.23it/s]\n",
      "Action: emotion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÄÃ£ thoÃ¡t webcam.\n",
      " ÄÃ£ lÆ°u áº£nh cuá»‘i cÃ¹ng tá»« webcam: result_deepface\\deepface_webcam_20250720_215023.jpg\n",
      "\n",
      "=== DeepFace Face Detection ===\n",
      "1. Analyze from image\n",
      "2. Analyze from video\n",
      "3. Analyze from webcam\n",
      "4. Exit\n",
      " Káº¿t thÃºc chÆ°Æ¡ng trÃ¬nh.\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import os\n",
    "import datetime\n",
    "import platform\n",
    "\n",
    "def is_gui_available():\n",
    "    return platform.system() != \"Linux\" or os.environ.get(\"DISPLAY\") is not None\n",
    "\n",
    "def analyze_and_draw(frame):\n",
    "    try:\n",
    "        results = DeepFace.analyze(frame, actions=[\"age\", \"gender\", \"emotion\"], enforce_detection=False)\n",
    "        y_label = 30  \n",
    "        for face in results:\n",
    "            region = face.get(\"region\", {})\n",
    "            if not region:\n",
    "                continue\n",
    "            x = region.get(\"x\", 0)\n",
    "            y = region.get(\"y\", 0)\n",
    "            w = region.get(\"w\", 0)\n",
    "            h = region.get(\"h\", 0)\n",
    "\n",
    "            gender_data = face.get(\"gender\", {})\n",
    "            gender = max(gender_data, key=gender_data.get) if isinstance(gender_data, dict) else str(gender_data)\n",
    "            age = str(face.get(\"age\", \"?\"))\n",
    "            emotion = str(face.get(\"dominant_emotion\", \"?\"))\n",
    "            label = f\"Gender: {gender} | Age: {age} | Emotion: {emotion}\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (10, y_label), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "            y_label += 30 \n",
    "    except Exception as e:\n",
    "        print(\"Lá»—i phÃ¢n tÃ­ch gÆ°Æ¡ng máº·t:\", e)\n",
    "    return frame\n",
    "\n",
    "def create_output_dir():\n",
    "    os.makedirs(\"result_deepface\", exist_ok=True)\n",
    "    return \"result_deepface\"\n",
    "\n",
    "def analyze_from_image(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(\"KhÃ´ng tÃ¬m tháº¥y áº£nh:\", image_path)\n",
    "        return\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    result = analyze_and_draw(image)\n",
    "\n",
    "    if is_gui_available():\n",
    "        cv2.imshow(\"Result from image\", result)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    output_path = os.path.join(create_output_dir(), f\"deepface_{os.path.basename(image_path)}\")\n",
    "    cv2.imwrite(output_path, result)\n",
    "    print(f\" áº¢nh káº¿t quáº£ Ä‘Ã£ lÆ°u táº¡i: {output_path}\")\n",
    "\n",
    "def analyze_from_video(video_path):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(\"KhÃ´ng tÃ¬m tháº¥y video:\", video_path)\n",
    "        return\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"KhÃ´ng má»Ÿ Ä‘Æ°á»£c video.\")\n",
    "        return\n",
    "\n",
    "    output_dir = create_output_dir()\n",
    "    last_result = None\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        result = analyze_and_draw(frame)\n",
    "        last_result = result.copy()\n",
    "\n",
    "        if is_gui_available():\n",
    "            cv2.imshow(\"DeepFace video\", result)\n",
    "\n",
    "        if is_gui_available() and cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    if is_gui_available():\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    if last_result is not None:\n",
    "        filename = f\"deepface_video_lastframe_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, last_result)\n",
    "        print(f\" ÄÃ£ lÆ°u frame cuá»‘i cÃ¹ng tá»« video: {output_path}\")\n",
    "    print(\"ÄÃ£ káº¿t thÃºc phÃ¢n tÃ­ch video.\")\n",
    "\n",
    "def analyze_from_webcam():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"KhÃ´ng thá»ƒ má»Ÿ webcam.\")\n",
    "        return\n",
    "\n",
    "    output_dir = create_output_dir()\n",
    "    analyzed_frame = None\n",
    "\n",
    "    print(\"ðŸ“¡ Äang phÃ¢n tÃ­ch tá»« webcam... Nháº¥n 'e' hoáº·c 'ESC' Ä‘á»ƒ thoÃ¡t.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            results = DeepFace.analyze(frame, actions=[\"age\", \"gender\", \"emotion\"], enforce_detection=False)\n",
    "            y_label = 30\n",
    "            for face in results:\n",
    "                region = face.get(\"region\", {})\n",
    "                if not region:\n",
    "                    continue\n",
    "                x = region.get(\"x\", 0)\n",
    "                y = region.get(\"y\", 0)\n",
    "                w = region.get(\"w\", 0)\n",
    "                h = region.get(\"h\", 0)\n",
    "\n",
    "                gender_data = face.get(\"gender\", {})\n",
    "                gender = max(gender_data, key=gender_data.get) if isinstance(gender_data, dict) else str(gender_data)\n",
    "                age = str(face.get(\"age\", \"?\"))\n",
    "                emotion = str(face.get(\"dominant_emotion\", \"?\"))\n",
    "                label = f\"Gender: {gender} | Age: {age} | Emotion: {emotion}\"\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (10, y_label), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "                y_label += 30\n",
    "\n",
    "            analyzed_frame = frame.copy()\n",
    "        except Exception as e:\n",
    "            print(\" Lá»—i phÃ¢n tÃ­ch:\", e)\n",
    "\n",
    "        if is_gui_available():\n",
    "            cv2.imshow(\"DeepFace webcam\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27 or key == ord('e'):\n",
    "            print(\"ÄÃ£ thoÃ¡t webcam.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    if is_gui_available():\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    if analyzed_frame is not None:\n",
    "        filename = f\"deepface_webcam_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, analyzed_frame)\n",
    "        print(f\" ÄÃ£ lÆ°u áº£nh cuá»‘i cÃ¹ng tá»« webcam: {output_path}\")\n",
    "    else:\n",
    "        print(\" KhÃ´ng cÃ³ áº£nh nÃ o Ä‘Æ°á»£c lÆ°u.\")\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        print(\"\\n=== DeepFace Face Detection ===\")\n",
    "        print(\"1. Analyze from image\")\n",
    "        print(\"2. Analyze from video\")\n",
    "        print(\"3. Analyze from webcam\")\n",
    "        print(\"4. Exit\")\n",
    "        choice = input(\" Nháº­p lá»±a chá»n (1/2/3/4): \")\n",
    "\n",
    "        if choice == '1':\n",
    "            path = input(\" Nháº­p Ä‘Æ°á»ng dáº«n áº£nh: \")\n",
    "            analyze_from_image(path)\n",
    "        elif choice == '2':\n",
    "            path = input(\"ðŸŽžï¸ Nháº­p Ä‘Æ°á»ng dáº«n video: \")\n",
    "            analyze_from_video(path)\n",
    "        elif choice == '3':\n",
    "            analyze_from_webcam()\n",
    "        elif choice == '4':\n",
    "            print(\" Káº¿t thÃºc chÆ°Æ¡ng trÃ¬nh.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\" Lá»±a chá»n khÃ´ng há»£p lá»‡.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b86542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
